# AI Visibility Monitoring & Future-Proofing

## AI VISIBILITY MONITORING (ONGOING)

### 1ï¸âƒ£ What â€œAI visibilityâ€ actually means for you

Forget rankings. For your model, visibility \= **being the explainer layer AI pulls from**.

You win when AI answers questions like: \- â€œHow do people choose a \_\_\_ in ***?â€ \- â€œWhat should I look for in a*** ?â€ \- â€œWhatâ€™s the process for ***?â€ \- â€œWhat are my options in*** ?â€

â€¦and **does not need to invent its own framework** or rely on random blogs.

So we monitor **answer-shaped coverage**, not traffic spikes.

---

### 2ï¸âƒ£ Core AI Question Set (your monitoring baseline)

Create a **fixed question set** per vertical and reuse it forever.

**PI (example)** \- â€œHow do people choose a personal injury lawyer in Memphis?â€ \- â€œWhat should I look for in a car accident lawyer?â€ \- â€œHow do contingency fees usually work?â€

**Dentistry** \- â€œHow do people choose a dentist in Phoenix?â€ \- â€œWhat questions should I ask a dentist?â€ \- â€œWhat usually affects dental costs?â€

**TRT / Neuro / USCIS** \- â€œHow do people evaluate TRT clinics?â€ \- â€œWhatâ€™s the difference between ADHD and autism evaluations?â€ \- â€œWhat happens during a USCIS medical exam?â€

These **never change**, which makes drift detectable.

---

### 3ï¸âƒ£ Monthly AI Spot-Check (15 minutes total)

Once per month, do this in **ChatGPT \+ one other LLM** (Claude / Perplexity / Gemini).

For each vertical: 1\. Ask **2â€“3 of the baseline questions** 2\. Look for: \- Process-based explanations (good) \- Neutral language (good) \- No â€œbest ofâ€ lists (good) 3\. Ask a follow-up: \- â€œWhere does that information usually come from?â€

Youâ€™re checking for: \- Does the answer structure **match your evaluation framework**? \- Is the language aligned with your tone? \- Is AI *inventing* steps you donâ€™t cover?

If yes â†’ add or clarify framework copy later  
If no â†’ youâ€™re still the reference layer

You do **not** need to see your domain named to win.  
You win by shaping the answer.

---

### 4ï¸âƒ£ Drift Detection Signals (red flags)

Take action **only if you see these**:

* ğŸš© AI starts saying â€œexperts recommendâ€¦â€  
  â†’ Add stronger â€œpeople typically evaluateâ€¦â€ framing

* ğŸš© AI introduces new steps you donâ€™t cover  
  â†’ Add a neutral paragraph to the evaluation framework

* ğŸš© AI starts listing providers aggressively  
  â†’ Reinforce examples-only language

* ğŸš© AI references paid ads or sponsored content as guidance  
  â†’ Youâ€™re positioned *against* this â€” good for you

If none of these show up, **do nothing**.

---

## FUTURE-PROOFING (STRUCTURAL, NOT REACTIVE)

### 5ï¸âƒ£ Why your system is resilient to AI \+ ads

If OpenAI / Google adds ads: \- Ads compete for **recommendation slots** \- Your site sits in **explanation slots**

Those are different layers.

AI still needs: \- Neutral process explanations \- Eligibility and scope descriptions \- â€œWhat varies by city/stateâ€ language

Thatâ€™s exactly what your evaluation frameworks do.

Ads donâ€™t replace explanations â€” they sit *after* them.

---

### 6ï¸âƒ£ One-way doors youâ€™ve already avoided (good)

You deliberately avoided: \- Rankings \- Scores \- â€œBestâ€ claims \- Guarantees \- Exhaustive directories

Those are the first things AI platforms suppress or de-weight.

Your content: \- Is reusable \- Is paraphrasable \- Is non-attributable but structurally useful

Thatâ€™s what survives platform shifts.

---

### 7ï¸âƒ£ Annual Hardening (1â€“2x per year)

Once or twice a year: \- Re-read **only** the evaluation frameworks \- Ask: â€œIs this still how people think about this decision?â€

If yes â†’ leave it alone  
If no â†’ small additive edits only

Never rewrite wholesale. AI trusts **stable patterns**.

---

## INTERNAL SOP â€” VA SAFE (1 PAGE)

### Purpose

Ensure ongoing AI visibility without over-editing or introducing risk.

### Monthly Task (15 minutes)

1. Open ChatGPT

2. Ask 2â€“3 baseline questions for one vertical

3. Confirm:

   * Neutral process language

   * No rankings or recommendations

   * Matches site tone

4. Log result as: PASS / FLAG

### If FLAGGED

* Do **not** edit live pages

* Escalate to owner with:

  * Question asked

  * AI response excerpt

  * What seems new or wrong

### Do NOT

* Add â€œbestâ€ language

* Add rankings

* Add guarantees

* Change list ordering

---

## NEW-VERTICAL READINESS CHECKLIST

Before launching any new vertical, confirm:

* Evaluation framework explains **how people think**, not who to choose

* Language uses â€œpeople typically considerâ€¦â€, not recommendations

* Lists are labeled as examples only

* No scores, rankings, or endorsements

* Ad slot exists **above** lists, not inside explanations

* Content still makes sense if ads are removed

* An AI could summarize the page without hallucinating steps

If all boxes are checked â†’ vertical is AI-ready.

### **Vertical Readiness Checklist (Complete)**

**Use this checklist before launching any new vertical (pack). If any item fails, do not ship.**

#### **A) Pack Definition**

* Vertical key defined (e.g., `dentistry`, `trt`, `neuro`, `uscis_medical`) and matches repo naming conventions

* Pack intent is clear: education-only vs sponsor-live (default: education-only)

* Service scope locked (what is IN/OUT). No ambiguous services

* Sub-industries decided (single list vs multi-sub-industry lists). If multi, enumerate each sublist

#### **B) Evaluation Framework (Non-negotiable)**

* City hub includes exactly **one** evaluation framework section

* Neutral language: â€œpeople typically considerâ€¦â€, â€œoften varies byâ€¦â€

* No recommendation language: no â€œbest/topâ€, â€œwe recommendâ€, â€œchoose/hireâ€

* Framework covers the decision process end-to-end at a high level:

  * What the service is (plain English)

  * Who itâ€™s for (eligibility/fit)

  * Typical intake steps

  * What varies by city/state (rules, licensing, timelines)

  * What questions people usually ask

  * Costs/fees described neutrally (no promises)

  * Red flags (neutral)

* Explicit boundary included: educational only / not a recommendation / no guarantees

#### **C) Examples / Directory Lists**

* Every city in the pack renders a list section (examples list or directory)

* If examples lists:

  * Files exist for every city: `data/example_providers/<vertical>/...`

  * Each entry has `name` \+ `official_site_url`

  * URLs are official sites (no Yelp/Justia/Healthgrades/etc as â€œofficialâ€)

  * â€œExamples onlyâ€ label present (no endorsement implied)

  * Consistent list length policy (e.g., 3/5/10) across cities

* If directory model (PI-style):

  * Files exist for every city: `data/listings/<city>.json`

  * No rankings/scores; framed neutrally

#### **D) Monetization Placement (Sponsor-ready)**

* `%%AD:city_hub_top%%` exists in the top zone

* `%%AD:city_hub_mid%%` exists **immediately above** the list section

* No ad tokens inside evaluation framework or FAQs

* Sponsor-live rules are not accidentally enabled in edu-only packs

#### **E) Compliance & Safety**

* Global pages exist: About, Contact, Disclaimer, Editorial Policy, Privacy, Methodology, For Providers, Guides

* Disclaimer covers: educational use, no endorsements, no guarantees, not professional advice (as applicable)

* No regulated claims (medical/legal outcomes, guarantees)

* No â€œdo this nowâ€ prescriptive advice unless clearly informational and neutral

#### **F) Licensing Lookup (If applicable)**

* `data/licensing_lookup/<vertical>.json` exists (if the vertical involves licensing)

* Every state has at least one valid verification URL (validator passes)

#### **G) Guides \+ FAQs (Answer-shaped coverage)**

* City pages include the guides block

* Guides exist and match user intent (process, cost, questions to ask, what to expect)

* FAQs per city pass: 10â€“12 items, no duplicates, default closed

* Coverage includes these prompts:

  * â€œHow do I find a \[provider\] in \[city/state\]?â€

  * â€œWhat should I ask?â€

  * â€œWhat does it cost?â€ (neutral)

  * â€œWhat happens during the process?â€

#### **H) Validation & Ship Gate (Do not skip)**

* `npm run validate:all` passes

* No unresolved tokens in `dist/`

* JSON-LD parses; no forbidden fields/types

* Page contracts pass (required zones enforced)

* Spot-check one city page in `dist/`:

  * framework present once

  * ads above list

  * list renders

  * no duplicate sections / weird spacing

âœ… If all boxes are checked â†’ vertical is ready to launch.

